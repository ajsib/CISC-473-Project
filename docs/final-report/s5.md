# S5 — Metric Evaluation (Pixel, Perceptual, Identity)

## Executive Summary

S5 quantifies how well each restoration method reconstructs the original aligned faces.

It does four things:

1. Joins ground-truth, degraded, and restored image paths into a single, consistent evaluation table.
2. Computes pixel-level metrics (PSNR, SSIM) between restored images and ground truth.
3. Computes perceptual similarity (LPIPS) and identity preservation (ArcFace cosine) between restored and ground truth.
4. Writes per-sample and aggregated metric tables for all methods, degradations, and CodeFormer `w` values.

S5 is pure evaluation. It does not modify images or manifests. Its outputs are the numeric backbone of the final comparison.

---

## Given Structures

### Inputs

From earlier stages:

* Aligned ground-truth images:

  * `data/processed/aligned/<id>.jpg`

* S3 manifest:

  * `results/logs/s3_degrade_manifest.csv`
  * Columns:

    * `id`
    * `path_gt`
    * `path_deg`
    * `degradation`
    * `split`

* S4A manifest (GFPGAN):

  * `results/logs/s4_gfpgan_manifest.*`
  * At minimum:

    * `id`
    * `path_gt`
    * `path_deg`
    * `path_restored`
    * `degradation`
    * `split`
    * `method = "gfpgan"`

* S4B manifest (CodeFormer):

  * `results/logs/s4_codeformer_manifest.*`
  * At minimum:

    * `id`
    * `path_gt`
    * `path_deg`
    * `path_restored`
    * `degradation`
    * `split`
    * `method = "codeformer"`
    * `w` (fidelity knob)

From configuration:

* `config.json.evaluation.paired_metrics`

  * `["PSNR", "SSIM", "LPIPS"]`

* `config.json.evaluation.unpaired_metrics`

  * `["ArcFaceCosine"]`

* `config.json.evaluation.tables.dir`

  * Base path for metric CSV outputs, e.g. `results/tables/`.

S5 assumes all referenced files from these manifests exist and are 256×256 aligned images.

---

## Transformations and Checks

S5 operates in two phases: table construction, then metric computation.

### 1. Evaluation table construction

1. **Manifest merge**

   * Load S3, S4A, and S4B manifests.

   * Construct a unified evaluation table where each row represents a single restored sample with:

     * `id`
     * `method` ∈ {`"gfpgan"`, `"codeformer"`}
     * `degradation`
     * `split`
     * `w` (only for CodeFormer rows; `null` or omitted for GFPGAN)
     * `path_gt`
     * `path_deg`
     * `path_restored`

   * Ensure that:

     * Every restored sample has an associated GT (from S3/S4 manifests).
     * No duplicate `(id, method, degradation, w)` entries.

   * Failure mode: missing `path_gt` or `path_restored` → log and abort.

2. **Optional subset selection**

   * Based on config or a simple rule, optionally restrict evaluation to a subset:

     * e.g. specific split (test only), specific degradations, or specific `w`.
   * This selection is documented in logs and not hard-coded.

3. **Pre-check of file shapes**

   * For a small sample of rows (or all, if cheap), open `path_gt` and `path_restored`:

     * confirm 256×256 size
     * confirm valid pixel range and number of channels.
   * Any violations are logged, and such rows are either dropped or cause stage failure, depending on strictness.

### 2. Metric computation

For each row in the evaluation table:

1. **Load images and normalization**

   * Read GT and restored images into arrays/tensors.
   * Normalize to the expected ranges:

     * for PSNR/SSIM: typically [0, 1] or [0, 255] consistent across both.
     * for LPIPS: normalized and resized as required by the backbone network.
     * for ArcFace: normalized and resized to network input requirements.

2. **Pixel metrics (paired)**

   * Compute PSNR(GT, restored).
   * Compute SSIM(GT, restored) using a standard window and configuration.
   * Store results as scalar floats.

3. **Perceptual metric (paired)**

   * Use LPIPS (with configured backbone, e.g., AlexNet or VGG).
   * Compute LPIPS(GT, restored).
   * Lower LPIPS indicates closer perceptual similarity.

4. **Identity metric (unpaired)**

   * Pass GT and restored images separately through a pretrained ArcFace backbone.
   * Obtain embeddings `f_gt` and `f_restored`.
   * Compute cosine similarity:

     * `cos(θ) = (f_gt · f_restored) / (‖f_gt‖ ‖f_restored‖)`
   * This quantifies identity preservation; higher is better.

5. **Row-level metric record**

   For each row, append:

   * `psnr`
   * `ssim`
   * `lpips`
   * `arcface_cosine`

   to the evaluation table.

6. **Aggregation**

   * Group the fully populated table by:

     * `method`
     * `degradation`
     * `w` (for CodeFormer)
     * optionally `split`

   * For each group and each metric, compute:

     * mean
     * standard deviation
     * count `n`

   * This produces the compact summary used for charts and report tables.

---

## Outputs and End State

S5 writes metric tables into `results/tables/`:

1. **Per-sample table** (detailed):

   * `results/tables/metrics_detailed.csv` (or similar)
   * Columns:

     * `id`
     * `method`
     * `degradation`
     * `split`
     * `w` (nullable)
     * `psnr`
     * `ssim`
     * `lpips`
     * `arcface_cosine`

2. **Aggregated summary**:

   * `results/tables/metrics_summary.csv`
   * Columns (as defined in `config.json.evaluation.tables.schema_csv` plus metric names):

     * `method`
     * `preset` (degradation)
     * `metric`
     * `mean`
     * `std`
     * `n`
     * optionally `w` for CodeFormer subsets.

3. **Optional split-specific tables**:

   * e.g., `metrics_summary_test.csv` if evaluation is limited to a certain split.

4. **Logs**:

   * `results/logs/s5_metrics.log` records:

     * total rows evaluated
     * any rows dropped and reasons
     * aggregation groups and sizes
     * completion status.

Downstream use:

* S6 (figures) reads `metrics_summary.csv` to produce:

  * bar charts for PSNR/SSIM across methods and presets
  * LPIPS vs ArcFace trade-off curves as a function of `w`.
* The final report directly references these tables for numeric comparisons between GFPGAN and CodeFormer.

S5’s functional role is to convert image-level restoration outputs into a rigorous, multi-metric comparison over methods, degradations, and fidelity settings.
