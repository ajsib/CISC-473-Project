# S1 — Data Ingestion and Verification

## Executive Summary

S1 asserts that the core dataset contract is satisfied before any downstream work happens.

It does three things:

1. Confirms that the CelebA aligned images and all required metadata CSVs physically exist in the expected locations.
2. Verifies basic internal consistency: IDs, counts, and splits line up across images and CSVs.
3. Produces a concise, machine- and human-readable summary of the dataset state in the logs.

S1 does not modify any data files, does not create new datasets, and does not depend on any models. If S1 passes, downstream stages can assume the dataset is present, well-formed, and immutable. If S1 fails, the pipeline stops immediately.

---

## Given Structures

S1 operates over a small, fixed set of filesystem and configuration “givens”.

### Dataset Layout (Filesystem Inputs)

* `data/img_align_celeba/`
  Directory of aligned face images (JPEG). This is treated as the canonical ground-truth image set.

* `data/list_attr_celeba.csv`
  Per-image attributes; row keys are image filenames.

* `data/list_bbox_celeba.csv`
  Bounding boxes for each face; row keys are image filenames.

* `data/list_eval_partition.csv`
  Integer partition labels (0/1/2) encoding train/val/test splits; row keys are image filenames.

* `data/list_landmarks_align_celeba.csv`
  Five-point landmarks (eyes, nose, mouth corners) for each image; row keys are image filenames.

These files are assumed to be fixed for the entire project. No stage is allowed to mutate or overwrite them.

### Configuration View

From `config.json`, S1 reads:

* `data.roots` — logical roots for raw/processed/historical data (used only to confirm that this particular run is “CelebA-only”).
* `data.manifests.schema_csv` — the expected column layout for synthetic manifests (used to establish the contract S3 must later follow, but not written by S1).
* `stack.determinism` — seeds and flags used for logging provenance, not for any stochastic computation in S1.

S1 does not branch on experiment matrix or model configuration. Its role is strictly data validation.

---

## Transformations and Checks

S1 applies a sequence of deterministic checks. There is no stochastic behavior and no write-back into `data/`.

1. **Filesystem presence check**

   * Verify that `data/img_align_celeba/` exists and is a directory.
   * Verify that all four CSV metadata files exist.
   * Failure mode: missing directory or file → log error → terminate pipeline.

2. **Image set scan**

   * Enumerate JPEG files under `data/img_align_celeba/`.
   * Record:

     * total image count
     * a few sample filenames for debugging
   * This establishes the universe of valid image IDs for later cross-checks.

3. **CSV schema validation**
   For each CSV:

   * Load only headers and a small row subset.
   * Ensure that required columns are present (e.g., filename / image_id, attribute columns, bbox columns, partition label, landmark coordinates).
   * Failure mode: missing or malformed columns → log explicit schema error → terminate.

4. **ID consistency checks**

   * Extract the key identifier (filename or image_id) from each CSV.
   * Compare against the set of filenames in `img_align_celeba/`.
   * Check:

     * All CSV IDs are a subset of existing image filenames.
     * Optionally, that images with no corresponding metadata remain below a small tolerance (or zero if strict).
   * Failure mode: any CSV referencing non-existent images or severe mismatch → log mismatch details → terminate.

5. **Partition sanity check**

   * From `list_eval_partition.csv`, confirm that:

     * only allowed partition labels (0, 1, 2) are present
     * each image in the CSV has exactly one partition label
   * Optionally compute counts per partition for the summary.
   * Failure mode: invalid labels or duplicate entries → terminate.

6. **Light coverage check across CSVs**

   * Confirm that the intersection of IDs across all four CSVs is non-empty and large (i.e., the dataset is not partially corrupted).
   * Log the counts of:

     * images present in all four CSVs
     * images missing from one or more CSVs (if any).
   * Depending on policy, either tolerate small discrepancies with a warning or enforce strict equality.

7. **Summary logging**

   * Write a compact S1 summary into:

     * `results/logs/pipeline.log` (global)
     * an S1-specific log (e.g., `results/logs/s1_data.log`)
   * Contents:

     * total image count
     * counts per partition
     * whether each CSV passed schema and ID checks
     * any warnings about minor discrepancies.

There is no data transformation beyond these checks; all operations are reads and comparisons.

---

## Outputs and End State

If S1 completes successfully:

* The `data/` directory is unchanged.
* `results/logs/` contains:

  * an updated global log with S1 entries
  * an S1-specific log section that describes:

    * dataset size
    * partition distribution
    * presence and validity of all metadata files
* The pipeline has a boolean invariant: “dataset is present and internally consistent”.

Downstream stages (S2–S7) can rely on:

* `img_align_celeba/` as a stable source of ground-truth face images.
* All four CSVs being present and structurally valid.
* Image IDs and splits being consistent across these files.

S1’s sole functional role is to enforce this invariant.
