# S1 — Data Ingestion and Verification

## Executive Summary

S1 asserts that the core dataset contract is satisfied before any downstream work happens.

It does three things:

1. Confirms that the CelebA aligned images and all required metadata CSVs physically exist in the expected locations.
2. Verifies basic internal consistency: IDs, counts, and splits line up across images and CSVs.
3. Produces a concise, machine- and human-readable summary of the dataset state in the logs.

S1 does not modify any *original* data files and does not depend on any models.
In the updated pipeline, S1 **does** create a validated-and-optionally-pruned working dataset written under:

**`results/outputs/s1-validated-pruned-dataset/`**

If S1 passes, downstream stages can assume the dataset is present, well-formed, and immutable for the remainder of the run. If S1 fails, the pipeline stops immediately.

---

## Given Structures

S1 operates over a small, fixed set of filesystem and configuration “givens”.

### Dataset Layout (Filesystem Inputs)

* `data/img_align_celeba/`
  Directory of aligned face images (JPEG). Treated as canonical ground-truth.

* `data/list_attr_celeba.csv`
  Per-image attributes.

* `data/list_bbox_celeba.csv`
  Bounding boxes.

* `data/list_eval_partition.csv`
  Integer partition labels (0/1/2).

* `data/list_landmarks_align_celeba.csv`
  Five-point landmarks.

These files are **never** mutated. They are strictly read-only for all stages.

### Configuration View

From `config.json`, S1 reads:

* `data.roots` — logical references for raw data.
* `data.manifests.schema_csv` — structural expectations for later synthetic manifests.
* `stack.determinism` — seeds and flags recorded for provenance.

S1 performs no branching on experimental methods or model settings.

---

## Transformations and Checks

S1 performs deterministic validation steps only. All operations are read-only against `data/`.

1. **Filesystem presence check**

   * Verify the existence of `data/img_align_celeba/`.
   * Verify all four CSVs exist.
   * Missing files → log error → terminate.

2. **Image set scan**

   * Enumerate JPEGs under `data/img_align_celeba/`.
   * Record total count and a few sample filenames.

3. **CSV schema validation**

   * Check that required columns exist for each CSV.
   * Malformed or missing columns → terminate.

4. **ID consistency checks**

   * Compare image filenames with each CSV’s image_id column.
   * Ensure all CSV IDs refer to real images.
   * Any mismatch → terminate.

5. **Partition sanity check**

   * Confirm labels ∈ {0,1,2}.
   * Confirm exactly one label per image.
   * Invalid or missing labels → terminate.

6. **Cross-CSV coverage check**

   * Compute intersection of IDs across the four CSVs.
   * Log counts for debugging and sanity.

7. **Summary logging**

   All validity findings are written to:

   * `results/logs/pipeline.log`
   * `results/logs/s1_data.log`

8. **Validated + Optional Pruned Dataset Output (New)**

   After validation, S1 now writes a reproducible working dataset to:

   **`results/outputs/s1-validated-pruned-dataset/`**

   This directory may contain:

   * A full copy of the validated dataset **or**
   * A size-limited pruned subset (when the user selects pruning).

   The structure mirrors the original CelebA layout but represents the dataset snapshot that all downstream stages operate on.

---

## Outputs and End State

If S1 completes successfully:

* The original `data/` directory remains unchanged.

* The validated working dataset is written to:

  **`results/outputs/s1-validated-pruned-dataset/`**

* Logs in `results/logs/` include:

  * S1 entries in the global pipeline log
  * A detailed S1-specific log (`s1_data.log`)

S1 guarantees the following invariant for downstream stages (S2–S7):

* The working dataset under
  **`results/outputs/s1-validated-pruned-dataset/`**
  is internally consistent, complete, and safe for all subsequent processing.

S1’s functional role is unchanged conceptually, but now explicitly materializes the validated (and optionally pruned) dataset for the rest of the pipeline.
